{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Plant Classification\n","\n","### Oumaima KHADIRA - Sarah ASSAF "]},{"cell_type":"markdown","metadata":{},"source":["In our project we will give an overall view of the models and methods used to handle a plant classification problem of 153 classes, we will show the intermediate work we did in order to get the final and best accuracy."]},{"cell_type":"markdown","metadata":{},"source":["#### 1. Data\n","   * Import libraries\n","   * Files path initialization\n","   * Data exploration\n","        * Data visualization\n","        * Problems with the datasets\n","        * Classes distribution\n","   * Data augumentation\n","   * Image preprocessing\n","   * Definition of dataset\n","   * Data loaders\n","   \n","#### 2. Modeling\n","   * Hyperparameters\n","        * Optimizer\n","        * Crieterion\n","        * Batch size\n","        * Learning rate\n","   * Pretrained models and results\n","        * Resnet18\n","        * Vgg19\n","        * Alexnet\n","        * Googlenet\n","        * Efficientnet b5\n","        * Efficientnet b0\n","   \n","#### 3. Training\n","   * Training loop\n","   * Training model\n","   * Generate output csv files\n","   \n","#### 4. Testing\n","   \n","#### 5. Conclusion"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Data"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1. Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install efficientnet_pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import display #We need the display function from IPython for Jupyter Notebook/Colab\n","from matplotlib.pyplot import imshow\n","from PIL import Image\n","from torchvision import  transforms\n","from efficientnet_pytorch import EfficientNet\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import os\n","import tqdm.auto as tqdm\n","import time\n","import torch.nn.functional as F\n","import cv2\n","import glob\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2. Files path initialization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["BASE_TRAIN_URL = \"/kaggle/input/polytech-nice-data-science-course-2021/polytech/train\"\n","BASE_TEST_URL = \"/kaggle/input/polytech-nice-data-science-course-2021/polytech/test\""]},{"cell_type":"markdown","metadata":{},"source":["### 1.3. Data exploration"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.3.1. Data visualization\n","We display 10 images randomly with their labels just to get an idea of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SCOPE_LIMIT = 11\n","\n","for i in range(1,SCOPE_LIMIT):\n","    \n","    train_path = BASE_TRAIN_URL + f\"/{i}\"\n","    ctr = 0\n","    for dirname, _, filenames in os.walk(train_path):\n","        \n","        for filename in filenames:\n","            if ctr >= 1:\n","                break\n","            print(f\"This is an example of image type : {i}\")\n","            pil_im = Image.open(os.path.join(dirname, filename), 'r').convert('RGB')\n","            display(pil_im)\n","            ctr = ctr + 1"]},{"cell_type":"markdown","metadata":{},"source":["![](![Screenshot (20).png](attachment:ae7bae4f-59af-41fa-9f41-9c3100613ad7.png))"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.3.2. Problems with the datasets\n","After exploring the dataset we found various aspects that will negatively affect the performance:"]},{"cell_type":"markdown","metadata":{},"source":["1. There are images which does provide a clear idea about the plant in question. \n","For example, we can notice in the images below that only a trunk of the tree is shown. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img=[Image.open('/kaggle/input/polytech-nice-data-science-course-2021/polytech/train/94/03aa4199e7a524b1347d6ff6a71624fb72e0a458.jpg'), \n","     Image.open('/kaggle/input/polytech-nice-data-science-course-2021/polytech/train/105/31f865004049633b61df3728fe14cd644e6b26f7.jpg')]\n","\n","display(*img)"]},{"cell_type":"markdown","metadata":{},"source":["![](![Screenshot (21).png](attachment:6f5b3641-82d6-4c6f-a49f-268056af15c9.png))"]},{"cell_type":"markdown","metadata":{},"source":["2. Some plants are dry, which gives a different visualization of the true plant."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img=Image.open('/kaggle/input/polytech-nice-data-science-course-2021/polytech/train/13/05afa0142c4ee1e85cab3983f7917238862d1880.jpg')\n","\n","display(img)"]},{"cell_type":"markdown","metadata":{},"source":["![](![Screenshot (22).png](attachment:815a85b5-b67d-4980-a3aa-bbfce223ff25.png))"]},{"cell_type":"markdown","metadata":{},"source":["3.There are some plants that have not yet flowered."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img=Image.open('/kaggle/input/polytech-nice-data-science-course-2021/polytech/train/1/03e33a2552a32be78ce4de23a432ab6b221e6717.jpg')\n","display(img)"]},{"cell_type":"markdown","metadata":{},"source":["![](![Screenshot (23).png](attachment:85ddd9c8-0a57-4e1a-935e-8df82e2b6148.png))"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.3.3. Classes distribution:"]},{"cell_type":"markdown","metadata":{},"source":["We are curious to see the distribution of each class of our dataset, so we can have an idea from where to begin our work.\n","\n","Note that all images in train/ or test/ are of .jpg extension"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(os.listdir(\"../input\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_image_names = glob.glob('../input/polytech-nice-data-science-course-2021/polytech/train/*/*.jpg') \n","print(\"Total number of training images: \", len(train_image_names))\n","\n","#make train_image_names as serie objectÂ¶\n","train_image_names = pd.Series(train_image_names)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train_df: a dataframe with 2 field: Filename, ClassId\n","train_df = pd.DataFrame()\n","\n","# generate Filename field\n","train_df['Filename'] = train_image_names.map(lambda img_name: img_name.split(\"/\")[-1])\n","\n","# generate ClassId field\n","train_df['ClassId'] = train_image_names.map(lambda img_name: int(img_name.split(\"/\")[-2]))\n","\n","class_id_distribution = train_df['ClassId'].value_counts()\n","class_id_distribution.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(60,20))\n","plt.xticks(np.arange(154))\n","plt.bar(class_id_distribution.index, class_id_distribution.values, color=(0.8, 0.2, 0.4, 0.8))"]},{"cell_type":"markdown","metadata":{},"source":["![](![Screenshot (27).png](attachment:e8f7f41d-7b1c-4ad8-9898-b5e54432e487.png))"]},{"cell_type":"markdown","metadata":{},"source":["As we can see in the previous figures the Dataset is unbalanced since in the training dataset we can find classes with more than 6000 samples (i.e. class 11) and classes with less than 120 samples (i.e. class 1)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.4. Data augmentation"]},{"cell_type":"markdown","metadata":{},"source":["When exploring our data we noticed that there are classes having a limited number of images, when others have much more examples, so we decided to use a data augmentation on the classes with fewer data examples so we can  increase the number of images trained by the network and in order to reduce the skews in data (one class having more data than the other). To do that we randomly resize, flip and crop the images so we can get many new versions of a single image and apply a different random transformation on each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocess= transforms.Compose([transforms.Resize((256,256)),\n","                              transforms.CenterCrop(224),\n","                              transforms.ToTensor(),\n","                            transforms.RandomHorizontalFlip(),\n","                            transforms.RandomVerticalFlip(),\n","                            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])])\n","\n","image_transforms = {\n","            'train':preprocess,\n","            'val': preprocess,\n","            'test':preprocess\n","            } "]},{"cell_type":"markdown","metadata":{},"source":["### 1.5. Image Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def imshow_tensor(image, ax=None, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","\n","    if ax is None:\n","        fig, ax = plt.subplots()\n","\n","    # Set the color channel as the third dimension\n","    image = image.numpy().transpose((1, 2, 0))\n","\n","    # Reverse the preprocessing steps\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    image = std * image + mean\n","\n","    # Clip the image pixel values\n","    image = np.clip(image, 0, 1)\n","\n","    ax.imshow(image)\n","    plt.axis('off')\n","\n","    return ax, image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def imshow(image):\n","    \"\"\"Display image\"\"\"\n","    plt.figure(figsize=(6, 6))\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.show()\n","\n","\n","# Example image\n","x = Image.open('/kaggle/input/polytech-nice-data-science-course-2021/polytech/train/101/032677244d114016482145eae92ef64579fbb080.jpg')\n","np.array(x).shape\n","imshow(x)"]},{"cell_type":"markdown","metadata":{},"source":["![](![Screenshot (28).png](attachment:ee7ad0a3-cf07-46c4-aa61-934c3ae1f0a8.png))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t = image_transforms['train']\n","plt.figure(figsize=(16, 16))\n","\n","for i in range(16):\n","    ax = plt.subplot(4, 4, i + 1)\n","    _= imshow_tensor(t(x), ax=ax)\n","\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["![](![Screenshot (29).png](attachment:c02d0687-1f5a-461c-87d8-d62844182686.png))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Unfortunately, this technique did not work with us since it gives a greater accuracy when using the old dataset without the augmentation. This is maybe because the most tested classes are the ones trained the most by the network.\n","So we decided not to use it and keep the data without augmentation, since we are searching for the better accuracy\n","Instead of that, we implemented a new method where where we make just a few transformations on the images of our network using image transforms. The operations were to resize the images to 299 x 299 and normalize them by subtracting the mean and dividing by the standard deviation. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocess = transforms.Compose([\n","    transforms.Resize(299),\n","    transforms.CenterCrop(299),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.6. Definition of our dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","  'Characterizes a dataset for PyTorch'\n","  def __init__(self, path_images=BASE_TRAIN_URL, transform=None, dataset_type='train'):\n","        'Initialization'\n","        \n","        # The attribute dataset is a dataframe with the data in csv defined by path_labels\n","        # The csv contains the name of images in the first column and their labels in the second\n","        \n","        self.label_start = 1\n","        self.label_end   = 153\n","        self.base_path_images = path_images\n","        self.transform = transform\n","        self.image_list = []\n","        self.dataset_type = dataset_type\n","        \n","        tmp_map = {}\n","        \n","        for dirname, _, filenames in os.walk(self.base_path_images):\n","            \n","            for filename in filenames:\n","                if '.jpg' in filename and '.comments' not in dirname:\n","                    if self.dataset_type == 'test':\n","                        label = os.path.join(dirname, filename).split('/')[-1].split('.')[0]\n","                    else:\n","                        label = int(os.path.join(dirname, filename).split('/')[-2])\n","                    full_filename = os.path.join(dirname, filename)\n","                    \n","                    if label not in tmp_map:\n","                        tmp_map[label] = []\n","                        \n","                    tmp_map[label].append(full_filename)\n","                    \n","        for l in sorted(list(tmp_map.keys())):\n","            if self.dataset_type == 'train':\n","                self.image_list += tmp_map[l][:int(0.8*len(tmp_map[l]))]\n","            if self.dataset_type == 'val':\n","                self.image_list += tmp_map[l][:int(0.8*len(tmp_map[l]))]\n","            if self.dataset_type == 'test':\n","                self.image_list += tmp_map[l]\n","\n","        del tmp_map\n","        \n","  def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.image_list)\n","\n","  def __getitem__(self, index):\n","        'Generates one sample of data'\n","\n","        # Load data and get label\n","        X = Image.open(self.image_list[index]).convert('RGB') # Open image at index and convert to RGB\n","        if self.dataset_type == 'test':\n","            y = self.image_list[index].split('/')[-1].split('.')[0]\n","        else:\n","            y = int(self.image_list[index].split('/')[-2]) - 1\n","        \n","        if(self.transform):\n","            X = self.transform(X) # Apply transformations on the image if defined\n","\n","        return X, y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set = Dataset(dataset_type = 'train', transform = preprocess)\n","val_set  = Dataset(dataset_type = 'val', transform = preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_set = Dataset(path_images = BASE_TEST_URL, dataset_type = 'test', transform = preprocess)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Modeling"]},{"cell_type":"markdown","metadata":{},"source":["For this project, we thought to try different neural network models on our plant dataset, and then choose the model that provides the best accuracy. We tried the models: resnet18, googlnet, alexnet, vgg19, efficientnet_b0, efficientnet_b5. We saw that the efficientnet_b0 model comes with the best acuuracy. In the following parts, we will show some results of some models we tried and our chosen settings for them."]},{"cell_type":"markdown","metadata":{},"source":["### 2.1. Hyperparameters\n","#### 2.1.1. Optimizer\n","We compared two of the most popular optimizers, ADAM and SGD, we found that ADAM had given a better accuracy than SGD (training accuracy = 0.049 for efficientnet_b0 in the first epoch, while it gives 0.754 for the same model, same epoch, with ADAM), so we chose ADAM as the optimizer.\n","#### 2.1.2. Criterion\n","The loss crieterion chosen is the negative log likelihood in PyTorch.\n","#### 2.1.3. Batch size\n","First we tried all the models with a batch size = 32, then we had increased it to 64, which gives almost better accuracies, we also tried batch_size = 128, but our machines were not able to do it because of a memory issue, so we kept the batch size 64.\n","#### 2.1.4. Learning rate\n","Learning rate is very important for an optimizer,the final goal is to acheive the global minimum of the loss function.\n","We tried the learning rate of 0.01,0.001 and 0.0001 as the starter learning rate. For the first one the result is very unstable and often can't get a loss that low enough. The learning rate 0.001 and 0.0001 have almost the same performance, but 0.001 are faster than 0.0001,so we choose the 0.001 as the learning rate."]},{"cell_type":"markdown","metadata":{},"source":["### 2.2. Pretrained models and results"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.1. Resnet18"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''def resNet():\n","    LEARNING_RATE = 0.001\n","    N_EPOCHS = 5\n","    \n","    # Define the neural network\n","    net = torchvision.models.resnet18(pretrained=True)\n","    set_parameter_requires_grad(net, True)\n","    num_ftrs = net.fc.in_features\n","    net.fc = nn.Linear(num_ftrs, 153)\n","    \n","    # Move model to the GPU\n","    net = net.cuda()\n","    \n","    # Define images transformations (pre processing)\n","    convert = torchvision.transforms.Compose([torchvision.transforms.Resize((299,299)), torchvision.transforms.ToTensor()])\n","\n","    # Negative log likelihood loss\n","    criterion = nn.CrossEntropyLoss().cuda()\n","\n","    # Stochastic Gradient Descent\n","    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","    \n","    return net, convert, criterion, optimizer'''"]},{"cell_type":"markdown","metadata":{},"source":["By launching the resnet18 model, with learning rate 0.001, batch size 64, optimizer ADAM, we obtained the following results:\n","\n","-epochs 0:\n","     Training accuracy: 0.5366598576046645 and Training loss: 0.12245032447465524\n","     \n","     Validation accuracy: 0.6700811137774428 and Validation loss: 0.0797195799488145\n","     \n"," -epoch1:\n","     Training accuracy: 0.6223945421732147 and Training loss: 0.0919386227157948\n","     \n","     Validation accuracy: 0.6836860163020844 and Validation loss: 0.07436497368758811\n","     \n"," -epoch2:\n","     Training accuracy: 0.6390337742696786 and Training loss: 0.08660303843129435\n","     \n","     Validation accuracy: 0.6951688713483926 and Validation loss: 0.07099304307477727"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.2. VGG19"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''def vgg19():\n","    N_EPOCHS = 20\n","    LEARNING_RATE = 0.001\n","    net = torchvision.models.vgg19(pretrained=True)\n","\n","    # Move model to the GPU\n","    net = net.cuda()\n","    \n","    # Define images transformations (pre processing)\n","    convert = transforms.Compose([\n","        transforms.Resize(299),\n","        transforms.CenterCrop(299),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","\n","    # Negative log likelihood loss\n","    criterion = nn.CrossEntropyLoss().cuda()\n","\n","    # Stochastic Gradient Descent\n","    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","    \n","    return net, convert, criterion, optimizer'''"]},{"cell_type":"markdown","metadata":{},"source":["By launching the vgg19 model, with learning rate 0.001, batch size 64, optimizer ADAM, we obtained the following results:\n","\n","-epochs 0:\n","     training accuracy: 0.0565218235357475755   and  training loss : 0.06833281091570115\n","     \n","     validation accuracy: 0.046080161830910495   and  validation loss     0.06843311045526965\n","     \n"," -epoch1:\n","     training accuracy: 0.05219840152311445   and  training loss : 0.068519341367136713217\n","     \n","     validation accuracy: 0.054677428951073914   and  validation loss     0.068398682803759571\n","     \n"," -epoch2:\n","     training accuracy: 0.05336850246911132   and  training loss : 0.0646130218859163\n","     \n","     validation accuracy: 0.054677428951073914  and  validation loss     0.068398682803759571"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.3. Alexnet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''def alexNet():\n","    N_EPOCHS = 20\n","    LEARNING_RATE = 0.001\n","    net = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n","\n","    # Move model to the GPU\n","    net = net.cuda()\n","    \n","    # Define images transformations (pre processing)\n","    convert = transforms.Compose([\n","        transforms.Resize(299),\n","        transforms.CenterCrop(299),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","\n","    # Negative log likelihood loss\n","    criterion = nn.CrossEntropyLoss().cuda()\n","\n","    # Stochastic Gradient Descent\n","    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","    \n","    return net, convert, criterion, optimizer'''"]},{"cell_type":"markdown","metadata":{},"source":["By launching the vgg19 model, with learning rate 0.001, batch size 64, optimizer ADAM, we obtained the following results:\n","\n","-epochs 0:\n","     training accuracy: 0.05526247942407235   and   training loss : 0.0689339786274436\n","     \n","     validation accuracy: 0.054677428951073914   and   validation loss     0.06843120239974558  \n","     \n"," -epoch1:\n","     training accuracy: 0.052723955337841855   and   training loss : 0.06847859151213531\n","     \n","     validation accuracy: 0.054677428951073914   and   validation loss     0.068398682803759571\n","     \n"," -epoch2:\n","     training accuracy: 0.05349741189536521   and  training loss : 0.06846043126650082\n","     \n","     validation accuracy: 0.047379172203161256   and  validation loss     0.06838812926981457  "]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.4. Googlenet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''def googleNet():\n","    LEARNING_RATE = 0.001\n","    N_EPOCHS = 9\n","\n","    net = torchvision.models.googlenet(pretrained=True)\n","\n","    # Move model to the GPU\n","    net = net.cuda()\n","    \n","    # Define images transformations (pre processing)\n","    convert = transforms.Compose([\n","        transforms.Resize(299),\n","        transforms.CenterCrop(299),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","\n","    # Negative log likelihood loss\n","    criterion = nn.CrossEntropyLoss().cuda()\n","\n","    # Stochastic Gradient Descent\n","    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","    \n","    return net, convert, criterion, optimizer'''"]},{"cell_type":"markdown","metadata":{},"source":["By launching the googlenet model, with learning rate 0.001, batch size 64, optimizer ADAM, we obtained the following results:\n","\n","-epochs 0:\n","     Training accuracy: 0.6685738651012435 and Training loss: 0.02038898156955875\n","     \n","     Validation accuracy: 0.7747853162247387 and Validation loss: 0.012348674944767131\n","     \n"," -epoch1:\n","     Training accuracy: 0.7839676338179006 and Training loss: 0.011803226653651466\n","     \n","     Validation accuracy: 0.8151637149713424 and Validation loss: 0.009688149456260503\n","     \n"," -epoch2:\n","     Training accuracy: 0.8197846220970589 and Training loss: 0.009557194838614082\n","     \n","     Validation accuracy: 0.8174146718759296 and Validation loss: 0.009569979449191622  "]},{"cell_type":"markdown","metadata":{},"source":["We also tried this model with the data augmentation maybe it will yields better results, but it gave a lower accuracy: 0.803862 for the training accuracy of the third epoch.\n"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.5. Efficientnet b5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''def efficientNet():\n","    LEARNING_RATE = 0.001\n","    N_EPOCHS = 20\n","\n","    net = EfficientNet.from_pretrained('efficientnet-b0')\n","\n","    # Move model to the GPU\n","    net = net.cuda()\n","    \n","    # Define images transformations (pre processing)\n","    convert = transforms.Compose([\n","        transforms.Resize(299),\n","        transforms.CenterCrop(299),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","\n","    # Negative log likelihood loss\n","    criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n","\n","    # Stochastic Gradient Descent\n","    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","    \n","    return net, convert, criterion, optimizer'''"]},{"cell_type":"markdown","metadata":{},"source":["By launching the efficientnet_b0 model, with learning rate 0.001, batch size 64, optimizer ADAM, we obtained the following results:\n","\n","-epochs 0:\n","     Training accuracy: 0.6719354262935565 and Training loss: 0.07986202374569976\n","     \n","     Validation accuracy: 0.740523091109318 and Validation loss: 0.00801569725346"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.6. Efficientnet b0 "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def efficientNet():\n","    LEARNING_RATE = 0.001\n","    N_EPOCHS = 20\n","\n","    net = EfficientNet.from_pretrained('efficientnet-b0')\n","\n","    # Move model to the GPU\n","    net = net.cuda()\n","    \n","    # Define images transformations (pre processing)\n","    convert = transforms.Compose([\n","        transforms.Resize(299),\n","        transforms.CenterCrop(299),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","\n","    # Negative log likelihood loss\n","    criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n","\n","    # Stochastic Gradient Descent\n","    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","    \n","    return net, convert, criterion, optimizer"]},{"cell_type":"markdown","metadata":{},"source":["By launching the efficientnet_b0 model, with learning rate 0.001, batch size 64, optimizer ADAM, we obtained the following results:\n","\n","-epochs 0:\n","     Training accuracy: 0.7541399758046923 and Training loss: 0.014889113296791838\n","     \n","     Validation accuracy: 0.830722091109216 and Validation loss: 0.009014848150113275\n","     \n"," -epoch1:\n","     Training accuracy: 0.8465581183190211 and Training loss: 0.008048727570332386\n","     \n","     Validation accuracy: 0.8671439620807965 and Validation loss: 0.006600792605357559\n","     \n"," -epoch2:\n","     Training accuracy: 0.8728258929456796 and Training loss: 0.006469210704176352\n","     \n","     Validation accuracy: 0.9054598100073379 and Validation loss: 0.00463159754480754  \n","     \n","     \n","We noticed that this model has given the best accuracy for the validation and the training so we applied it on 10 epochs, it gives the results for the epoch 10:\n","\n","Training accuracy: 0.9420800031731551 and Training loss: 0.0026503791539075876\n","\n","Validation accuracy: 0.9456597187791286 and Validation loss: 0.002409334887945289\n"]},{"cell_type":"markdown","metadata":{},"source":["Hence, we decided to apply it on 20 epochs for the final submission, hoping it will yields to better results, but again, we had a memory issue in our machines that we were not able to turn the model on more than 10 epochs, so we kept our final model the efficientnet_b0 with a training and validation accuracy of 94% after 10 epochs."]},{"cell_type":"markdown","metadata":{},"source":["![](![Screenshot (24).png](attachment:12e068fc-4a48-401c-91e0-eaae397e46a4.png))\n","\n","![](![Screenshot (25).png](attachment:6d54c888-bbed-4699-b8f7-5001c1c45538.png))\n","\n","![](![Screenshot (26).png](attachment:60b60e90-45cb-46fb-bbe7-8785f0d342a2.png))"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Training\n","For the training loop, we iterate using the train dataloader, at each time one batch in the model. Each time we send a batch of inputs through the model, we call it epoch, and we train our model for a precise number of epochs. \n","\n","At the end of the training loop, we record the loss and the accuracy for training and validation for each epoch in order to compare models and get the best results."]},{"cell_type":"markdown","metadata":{},"source":["### 3.1. Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_and_eval(net, criterion, optimizer, train_dl, val_dl, hidden=False):\n","    '''\n","    Train and evaluate a neural network\n","    -- Inputs:\n","    net: neural network\n","    criterion: loss function\n","    optimizer: optimizer function\n","    train_dl: train dataset data loader\n","    val_dl: val dataset data loader\n","    '''\n","    \n","    epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc = [], [], [], []\n","\n","    for e in range(N_EPOCHS):\n","        print(\"EPOCH:\", e)\n","\n","        ### TRAINING LOOP\n","        running_loss = 0\n","        running_accuracy = 0\n","\n","        # Put the network in training mode\n","        net.train()\n","\n","        for i, batch in enumerate(tqdm.tqdm(train_dl)):\n","\n","            # Get a batch from the dataloader\n","            x = batch[0]\n","            labels = batch[1]\n","\n","            # Move the batch to GPU\n","            x = x.cuda()\n","            labels = labels.cuda()\n","\n","            # Compute the network output\n","            if hidden:\n","                y, hidden = net(x)\n","            else:\n","                y = net(x)\n","\n","            # Compute the loss\n","            loss = criterion(y, labels)\n","\n","            # Reset the gradients\n","            optimizer.zero_grad()\n","\n","            # Compute the gradients\n","            loss.backward()\n","\n","            # Apply one step of the descent algorithm to update the weights\n","            optimizer.step()\n","\n","            # Compute some statistics\n","            with torch.no_grad(): # disables gradient calculations\n","                running_loss += loss.item()\n","                running_accuracy += (y.max(1)[1] == labels).sum().item()\n","\n","        print(\"Training accuracy:\", running_accuracy/float(len(train_set)),\n","              \"Training loss:\", running_loss/float(len(train_set)))\n","\n","        epoch_loss.append(running_loss/len(train_set))\n","        epoch_acc.append(running_accuracy/len(train_set))\n","\n","        ### VALIDATION LOOP\n","        # Put the network in validation mode\n","        net.eval()\n","\n","        running_val_loss = 0\n","        running_val_accuracy = 0\n","\n","        for i, batch in enumerate(tqdm.tqdm(val_dl)):\n","\n","            with torch.no_grad(): # disables gradient calculations\n","                # Get a batch from the dataloader\n","                x = batch[0]\n","                labels = batch[1]\n","\n","                # Move the batch to GPU\n","                x = x.cuda()\n","                labels = labels.cuda()\n","\n","                # Compute the network output\n","                if hidden:\n","                    y, hidden = net(x)\n","                else:\n","                    y = net(x)\n","\n","                # Compute the loss\n","                loss = criterion(y, labels)\n","\n","                running_val_loss += loss.item()\n","                running_val_accuracy += (y.max(1)[1] == labels).sum().item()\n","\n","        print(\"Validation accuracy:\", running_val_accuracy/float(len(val_set)),\n","              \"Validation loss:\", running_val_loss/float(len(val_set)))\n","\n","        epoch_val_loss.append(running_val_loss/len(val_set))\n","        epoch_val_acc.append(running_val_accuracy/len(val_set))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2. Training model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["N_EPOCHS=20"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["net, convert, criterion, optimizer = efficientNet()"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3. Generate output csv file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_network(network, test_dl, hidden=False):\n","    \n","    network.eval()\n","\n","    test_files_labels = []\n","    outputs           = []\n","\n","    for i, batch in enumerate(tqdm.tqdm(test_dl)):\n","\n","        with torch.no_grad(): # disables gradient calculations\n","            # Get a batch from the dataloader\n","            x = batch[0]\n","            labels = batch[1]\n","\n","            # Move the batch to GPU\n","            x = x.cuda()\n","            \n","\n","            # Compute the network output\n","            if hidden:\n","                y, hidden = net(x)\n","            else:\n","                y = net(x)\n","            \n","            test_files_labels += labels\n","            outputs += (list(y.cpu().numpy().argmax(axis=1) + 1 ) )\n","            \n","            if (i % 100) == 0: \n","                print(f\"batch {i} Done\")\n","            \n","    \n","    submission_df = pd.DataFrame({\"image_name\" : test_files_labels, \"class\" : outputs})\n","    submission_df['image_name'] = submission_df['image_name'].apply(lambda x : str(x)+'.jpg')\n","    submission_df.to_csv(\"submission_efficient.csv\", index=False)\n","    \n","    print(\"File was saved as 'submission_efficient.csv'\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_and_eval(net, criterion, optimizer, train_dl, val_dl)"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_network(net, test_dl)"]},{"cell_type":"markdown","metadata":{},"source":["In this part, we thought to make a confusion matrix, but then we decided not, for we have so much classes and the matrix would not be lisible to allow us compare the predicted and real classes of the images."]},{"cell_type":"markdown","metadata":{},"source":["## 5. Conclusion\n","Now, let's review the process of our project.\n","â\n","First, we used the materials we have learned and many others online, to be able to classify the plant images the better way. \n","â\n","To do that we tried different models from the PyTorch library suitable for image classification, and for that we learned so much. \n","While exploring the data we found that it was too large, so we tried the data augmentation, but when we applied the efficientnet model on the training set, we didn't notice a great change in the accuracy, neither with the googlenet model, so we continued without the augmentation.\n","â\n","Then we have studied and tested the models function and the different parameter settings in each one.\n","â\n","We faced a real problem with the memory, so a model like Xception did not be able to run, same for a large number of epochs and larger batch size. We beleive that these parameters may increase more our accuracy but unfortunately we could not.\n","â\n","After we get the result, we found that balancing the data before training was a good idea, but it was too late for us since we noticed that after we had trained a lot of models with unbalanced data, and for training all the models again will takes a lot of time. For sure in the next projects we are going to balance the data from the begining.\n","â\n","This project allowed us to apply the theoretical knowledge we have learned to deal with practical problems and deal with real data. Compared with simulated data, we need to try more methods for this kind of real data and do more parameter learning to achieve a good effect. So for real data, especially massive data, it takes a long time to select and optimize our model.\n","â\n","Finally we ended with an accuracy of 94%."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
